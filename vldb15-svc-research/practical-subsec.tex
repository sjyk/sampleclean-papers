\subsection{Error Bars and Optimality}\label{subsec:correct-practical}
\subsubsection{Confidence Intervals}
In Table \ref{tbl:query-correct}, we present formulas to correct stale query results.
In each of these formulas, all of the $\dans$ terms correspond to estimates, and these estimates need to be bounded.
To find these bounds, we can rewrite the terms $\dans$ as a mean value of random variables.
By the Central Limit Theorem, the mean value of numbers drawn by uniform random sampling $\bar{X}$ approaches a normal distribution with:
\[
\bar{X} \sim N(\mu,\frac{\sigma^2}{k}),
\]
where $\mu$ is the true mean, $\sigma^2$ is the variance, and $k$ is the sample size.\footnote{\scriptsize For sampling without replacement, there is an additional term of $\sqrt{\frac{N-k}{N-1}}$. We consider estimates where N is large in comparison to k making this term insignificant.}
We can use this to bound the term with its 95\% confidence interval (or any other user specified probability), e.g., $\bar{X} \pm 1.96 \frac{\sigma}{\sqrt{k}}$.

For all of the queries in \aggview, and for \sumfunc, \countfunc in \spview and \fjview, there is only one term to bound.
However, for \avgfunc in \spview and \fjview, as presented there are two terms to bound.
We can reformulate the formula into the following form:
\[
\frac{\dans_{sum} + A}{\dans_{cnt} + B}
\]
where A and B are constants, with $A = \rho \cdot \ans_{cnt} \cdot \ans_{avg}$ and $B = \rho \cdot \ans_{cnt}$.
Next, we notice that the reformulation is now a \sumfunc divided by a \countfunc of the same set of records, with additional constant offsets.
This can be written as a mean value of random variables, and we can bound this in confidence intervals.
See \cite{wang1999sample} for more details on the confidence intervals.

\subsubsection{Optimality}
We can prove that for the \sumfunc, \countfunc, and \avgfunc queries this estimate is optimal with respect to the variance.
\begin{proposition}
An estimator is called a minimum variance unbiased estimator (MVUE) of a parameter if it is unbiased and the variance of the parameter estimate is less than or equal to that of any other unbiased estimator of the parameter.
\end{proposition}
The concept of a Minimum Variance Unbiased Estimator (MVUE) comes from statistical decision theory \cite{cox1979theoretical}.
Unbiased estimators are ones that, in expectation, are correct.
However, on its own, the concept of an unbiased estimate is not useful as we can construct bad unbiased estimates.
For example, if we simply pick a random element from a set, it is still an unbiased estimate of the mean of the set.
The variance of the estimate determines the size of our confidence intervals thus is important to consider.

The \sumfunc, \countfunc, and \avgfunc queries are linear functions of their input rows.
We explored whether our estimate was optimal for linear estimates, for example, should we weight our functions when applied to the sample.
It turns out that the proposed corrections are the optimal strategy when nothing is known about the data distribution a priori.

\begin{theorem}
For \sumfunc, \countfunc, and \avgfunc queries, our estimate of the correction is optimal over the class of linear estimators when no other information is known about the distribution. 
In other words, there exists no other linear function of the set input rows $\{ X_i \}$ that gives a lower variance correction.
\end{theorem}
\begin{proof}[Sketch]
As discussed before, we reformulate \sumfunc, \countfunc, and \avgfunc as means over the entire table. We prove the theorem by induction. 
Suppose, we have two independent unbiased estimates of the mean $\bar{X}_1$ and $\bar{X}_2$ and we want to merge them into one estimate $\bar{X} = c_1\bar{X}_1+c_2\bar{X}_2$.
Since both estimates are unbiased, $c_1 + c_2 = 1$ to ensure the merged estimate is also unbiased, and now we consider the variance of the merged estimate.
The variance of the estimate is $var(\bar{X}) = c_1^2var(\bar{X}_1) + c_2^2var(\bar{X}_2)$.
Since, we don't know anything about the distribution of the data, by symmetry $var(\bar{X}_1) = var(\bar{X}_2)$.
Consequently, the optimal choice is $c_1=c_2=0.5$. 
We can recursively induct, and we find that if we do not know the variance of data, as is impossible with new updates, then equally weighting all input rows is optimal. 
\end{proof}
