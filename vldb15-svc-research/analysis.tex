\section{Extensions}\label{sec:ext}
\subsection{Hash-Operator}
We defined a concept of tuple-lineage with primary keys.
However, a curious property of the deterministic hashing technique is that we can actually hash any attribute while retain the important
statistical properties.
This is because a uniformly random sample of any attribute (possibly not unique) still includes every individual row with the same probability.  
A consequence of this is that we can push down the hashing operator through arbitrary equality joins (not just many-to-one) by hashing the join key.

We defer further exploration of this property to future work as it introduces new tradeoffs.
For example, sampling on a non-unique key, while unbiased in expectation, has higher variance in the size of the sample.
Happening to hash a large group may lead to decreased performance. 

Suppose our keys are duplicated $\mu_k$ times on average with variance $\sigma_k^2$, then the variance of the
sample size is for sampling fraction $m$:
\[m(1-m)\mu_k^2+(1-m)\sigma_k^2\]
This equation is derived from the formula for the variance of a mixture distribution.
In this setting, our sampling would have to consider this variance against the benefits of pushing the hash operator further down the query tree. 

\subsection{Sampling Updates vs. Sampling Views}
In SVC, we sample from views and work backwards through the view definition using relational algebra.
An alternative approach is to sample the base relations of the view.
However, this approach quickly leads to some bottlenecks.
For example, if our view is an aggregate view with a nested selection, we can easily construct a distinct count problem rendering any aggregate query inestimable \cite{DBLP:conf/pods/CharikarCMN00}.

For some types of views, this model is actually a special case of SVC.
For views where primary key of the base relations is an attribute of the view, we can sample those attributes.
We can quickly see that based on our pushdown rules if there is a nested aggregate query, pushdown can fail in general.
However, re-writing views and queries to better support sampling is an interesting avenue of future work.

\subsection{Multi-View Setting}
In a production environment, the database system might have many materialized views. 
With the sampling ratio, SVC gives the database administrator an additional degree of freedom to adjust throughput, storage, and accuracy.
The sampling ratio of each view can be adaptively adjusted to suit the workload.

We can pose minimizing the expected estimation error as a Geometric Convex Program.
In one time period, if view $i$ has an expected cardinality of $N_i$, an average query variance of $\alpha_i$, a sampling ratio of $m_i$, there is a total space budget of $B$, the cost for update is $C_i$ secs/Record and throughput demand of $D$ latency:
\[\arg \min_{m_i} \sum_i \frac{\alpha_i}{m_i \cdot N_i}\]
\[\text{subject to:} \sum_i m_i \cdot N_i \le B \]
\[\sum_i m_i\cdot N_i \cdot C_i \le D \]




