%\documentclass[english]{sig-alternate}

%\begin{document}

{\noindent \normalsize \bf Dear SIGMOD Chair and Referees: }

\vspace{1em}

Thank you for your detailed feedback.
%We revised the paper accordingly 
%and hope you will agree it is improved
%and now addresses the issues raised.  Detailed notes are below.
We have tried to address all concerns raised in the reviews. 
In the following, we will first summarize our major changes to the paper, and then respond to each of the individual comments respectively.


\vspace{0.5em}
{\noindent The major changes of the paper are as follows.} %\vspace{-0.5em}
\begin{itemize}

  \item The ``Black-box" Data-cleaning Component.
  
   %In the submitted paper we treated cleaning as a black box.  In the revised paper
%we open the box and add Section~2.3.1 that cites a variety of published cleaning methods, all of which
%can be applied with the SampleClean framework.
   %In the submitted paper we treated cleaning as a black box.  In the revised paper
To address this issue, we added two new subsections (2.3.1~and~2.3.2) that cite a variety of published cleaning methods, all of which can be applied with the SampleClean framework, and described how these methods can be used. 

  \item Perfect Cleaning Assumption. 

We added a new experiment in Section~\ref{exp:sensitivity} relaxing this assumption. The results show that our approach offers benefits even when cleaning is only 10\% effective.

  \item Lack of Clarity on Deduplication.
  
   We clarified this in the revision (Section~\ref{sec:duplication}). 
Deduplication is often a two phase procedure: coarsely blocking the data and matching within blocks.
While, we still need to scan the entire dataset to block the data, sampling can significantly reduce matching costs. 
  % \vspace{-.5em}



  %While it is true that this is a potentially expensive query to the entire dataset, we have detailed how this can still reduce computation in pairwise entity resolution systems. \vspace{-.5em}
%\iffalse
  \item Optimal Allocation. 

We followed the reviewer's suggestion and merged the former Section 6 with the future work section in a very concise form, giving us also more space to explain the other techniques in more detail. 

%We agree this section was not fully-fleshed out and requires more space to fully explore.  
%In the revision we removed it to provide
%space to address the other points raised.
% In the revision we summarized its ideas in the future work section and made the extra space to address the other points raised.

  \item Relation to SAQP.
  
   We clarified in the revision (Section~2.3.3, SampleClean v.s. SAQP) that SAQP assumes perfectly clean data, and SampleClean relaxes this assumption and makes cleaning feasible.
%\fi 
 

\iffalse
  \item Perfect Cleaning assumption. 

This is an excellent point.  We added an entire new experiment to explore imperfect
cleaning in Section~\ref{exp:sensitivity} that show the surprising result that the approach offers benefits even when cleaning is only 10\% effective.
\fi

\iffalse
  \item Relation to SAQP.
  
    Thank you for pointing this out.  Both SAQP
and SampleClean use sampling to estimate the value of aggregate
queries.  SAQP assumes that data is perfectly clean: the error and
associated confidences result from the sampling process.  SampleClean
relaxes this assumption.  In SampleClean, error and associated
confidence result from both dirtiness / inherent error in the data and
from the sampling process.  What is surprising about SampleClean is
that sampling a relatively small population of the overall data makes
it feasible to manually or algorithmically clean the sample, and
experiments confirm that this cleaning often more than compensates for
the error introduced by the sampling.  We agree in retrospect that
this was not made clear, and we clarify in the revised version.
\fi



%We clarified that SampleClean is a generic framework designed to scale expensive data cleaning procedures and is beneficial even when the cleaning is imperfect.  It allows the user to apply the data cleaning technique to a sample and returns results which in expectation are as if the entire dataset was cleaned.  While the ultimate result may differ from the true value, the experiment indicated that SampleClean with imperfect cleaning was more accurate than AllDirty (an aggregation of the entire dirty data) for a very small cleaning cost.

\end{itemize}
\vspace{-0.5em}

%To summarize, we clarified the assumptions and the nature of the "black-box" data cleaning discussed in our first revision.
%Firstly, SampleClean is a generic framework designed to scale expensive data cleaning procedures and is beneficial even when the cleaning is imperfect.
%It allows the user to apply the data cleaning technique to a sample and return results which in expectation are as if the entire dataset was cleaned.
%While the ultimate result may differ from the true value, we added an experiment where SampleClean with imperfect cleaning is more accurate than AllDirty for a very small cleaning cost.
 
%We cited a list of recent data cleaning techniques that can be applied in this framework, and gave detailed examples of how the system would work end-to-end.

%Another concern was the nature of the deduplication proposed in our first revision. 
%For each element in the sample, the data cleaning technique needs to estimate the number of times it is duplicated in the entire dataset.
%While it is true that this is a potentially expensive query to the entire dataset, we have detailed how this can still reduce computation in pairwise entity resolution systems.

%We provide a detailed response to each comment as follows: 

\vspace{1em}

{\noindent \bf Response to the comments of Referee 1.}

\vspace{1em}

\emph{W1. Section 2, beginning: The author (s) introduce an example without Group by. Since the paper also discusses group by aspects, I highly recommend using a query that includes a Group by clause!}

A: %Thank you for pointing out this inconsistency!  
Thank you for this suggestion. We changed the example to a query with a group-by clause.


\mbox{}

\emph{W2. I recommend merging Sections 2 and 3 and better integrating both content wise.}

A: In the revision, we have merged Sections 2 and 3, and provided a more detailed section overview in the preface of Section 2. We also revised Sections 2.1-2.3 accordingly to make them more clear.
In addition to the reorganization, we provided a list of recent data cleaning techniques that are applicable to our framework, and details on how they would be integrated.
We also provided a detailed example implementation of our framework, and added a subsection comparing and contrasting our approach with SAQP.

%We also added a subsection comparing and contrasting our approach with SAQP, and using duplication errors, we illustrated why our result estimation problem can be more challenging than SAQP.

\mbox{}

\emph{W3. Section 4, right column bottom: I really don't understand the Quotient (k/k(pred)) for the average Aggregate function. Is this due to a Group by?? Why does this term show up for average and not for Count and sum?}

A: %Sorry we were unclear on this.  
We added to the end of Section 3.1 an explanation of how we derive the Quotient (k/k(pred)) for the average query.
In short, we define k to be the sample size including records that do not satisfy the predicate, therefore, when we average over these records we have to correct the estimate by a factor of (k/k(pred)). 

%We added clarification to the section about all of the scaling factors. To summarize, we create a sample of the dataset (including tuples which may not satsify the predicate or have condition errors).
%Statistically, it is convenient to define our aggreagation over the whole sample not just tuples that satisfy the predicate (using a case statement to set the values to 0 when they do not satisfy the predicate). When we run the AVG query on this sample, it will be an underestimate as it includes these 0 values and we compensate by multiplying by k/k(pred). The SUM and COUNT queries are not affected by this, as a 0 value just means that that tuple is not contributing to the total.

\mbox{}

\emph{W4-1. While Sections 4 and 5 provide a good conceptual description of the Approach - including the necessary statistical Background/theory - I really miss how these two approaches are then "put into reality". Moreover, this gap is actually the biggest concern I have with this paper. I would like to be more specific:}

A1: 
%\tim{Within in the paper, please make the example a new subsection: Title:"Example: Publication Cleaning with OpenRefine"}
%
To address the practicality issue, we add a new sub-section (2.3.4), which describes by means of a use case, how one could use SampleClean with the popular OpenRefine tool for data cleaning. The subsection illustrates how cleaning a dataset can be very costly and that sampling can help reduce this cost. 
%\tim{We should talk about this one. I am a bit worried, that it is not enough. }
This example is similar to the manual techniques we applied to the Microsoft Academic Search Dataset in our experiments (Section 5.7.1).
\vspace{1em}

\emph{W4-2. It is unclear how to determine, for example, the duplication factor without going thru the complete relation. Or is the duplication factor also determined from the sample? Before the beginning of Subsection 4.2.1 you define that Numdup(t) is the duplication factor for P, the complete relation.... }

\emph{Are duplicates considered as many times as there are duplicates, i.e. every time when they show up in the relation? How does the duplication factor affect the number of tuples? Not at all? }



A2: %This was unclear in the submission and we have clarified carefully in the revision.  
%
%[Ken: this is still a bit unclear below and in the text.  If we use the sample to estimate the number of dups, then 
%there is error from such imperfect deduplication, and don't we explore this in the new Section \ref{exp:sensitivity}?]
%
The description of deduplication was not clear in the submission and we have clarified carefully in the revision.
In Section \ref{sec:duplication}, we added:
\newline

\noindent \textit{The \saqpplus framework defines the duplicate factor for a tuple as the number of times the tuple appears in the entire table. To determine it, one way would be to estimate its value from the sample. However, both analytical proofs and empirical tests have shown that this method can lead to highly inaccurate query results~\cite{charikar2000towards}.
Therefore, in our paper, we determine the duplication factor from the complete relation. 
} 


\iffalse
Duplicate detection or entity resolution has been extensively studied for several decades~\cite{DBLP:journals/tkde/ElmagarmidIV07}.
\saqpplus needs an estimate of the duplication rate for a tuple which we define as the number of times the tuple has been duplicated in the entire table.
It is known that estimating the duplication rate from only searching in a sample leads to inaccurate query results~\cite{charikar2000towards}.
Consequently, we cannot avoid a scan of the entire table to clean this error type.}
\fi

\textit{It is important to note, however, that compared to full cleaning, we only need to determine the duplication factor for those tuples in the sample. As with other uses of sampling, this can result in significant cost savings in duplicate detection.
In the following, we will describe how to apply existing deduplication techniques to compute the duplication factor, and explain why it is cheaper to determine the duplication factor for a sample of the data, even though doing so requires access to the complete relation.}


\textit{Duplicate detection (also known as entity resolution) aims to identify different tuples that refer to the same real-world entity. This problem has been extensively studied for several decades (see~\cite{DBLP:journals/tkde/ElmagarmidIV07} for a survey).  
Most deduplication approaches consist of two phases:} 

\begin{enumerate}\vspace{-.25em}
\item \emph{Blocking}. 
\textit{Due to the large (quadratic) cost of all-pair comparisons, data is partitioned into a number of blocks, and duplicates are considered only within a block. 
For instance, if we partition \texttt{papers} based on \texttt{conference\_name}, then only the papers that are published in the same conference will be checked for duplicates; \vspace{-.25em}}
%Note that we only have to perform blocking once and this can be done offline, e.g., when we are creating the sample.
%There are many effective techniques for block retrieval such as using a hash index~\cite{journals/tkde/Christen11}.
\vspace{-.25em}
\item \emph{Matching}. 
\textit{To decide whether two tuples are duplicates or not, existing techniques typically model this problem as a classification problem, and train a classifier to label each tuple pair as duplicate or non-duplicate~\cite{DBLP:conf/kdd/BilenkoM03}. 
In some recent research (and also at many companies) crowdsourcing is used to get humans to match tuples~\cite{DBLP:journals/pvldb/WangKFF12,DBLP:conf/www/DemartiniDC12}.}
\end{enumerate}\vspace{-.25em}

\textit{A recent survey on duplicate detection has argued that the matching phase is typically much more expensive than the blocking phase~\cite{journals/tkde/Christen11}. 
For instance, an evaluation of the popular duplicate detection technique~\cite{DBLP:conf/kdd/BilenkoM03} shows that the matching phase takes on the order of minutes for a dataset of thousands of tuples~\cite{journals/pvldb/KopckeTR10}.
This is especially true in the context of crowdsourced matching where each comparison is performed by a crowd worker costing both time and money.
\saqpplus reduces the number of comparisons in the matching phase, as we only have to match each tuple in the sample with the others in its block.
For example, if we sample 1\% of the table, then we can reduce the matching cost by a factor of 100.}
\newline

We also further clarify some confusing notation defined in Section~\ref{sec:sampleclean}. Above the Section~3.2.1, we clarify that \Dedup{t}{P} is the number of times that the tuple $t$ appears in the entire table. In the caption of Table~1, we clarify  that $N$ is the total number of tuples in the dirty data (including duplicates), which is not affected by the duplicate rate.  

\vspace{1em}

%The duplication factor of each tuple is defined as the number of duplicates of the tuple in the entire data. To determine it, one way is to estimate its value from the sample. However, both analytical proofs and empirical tests have shown that in some cases this method may lead to highly inaccurate results~\cite{charikar2000towards}.
%Therefore, in our work we determine the duplication factor from the complete relation. 
%But the key point is that instead of determining the duplication factor for every tuple in the entire data, we only need to determine the duplication factor for those tuples in the sample. 
%
%We will describe in the following that how to apply existing deduplication techniques to compute the duplication factor, and explain why it is cheaper to determine the duplication factor for a sample of the data, even if it requires access to the complete relation.

%\tim{This is the same text as in the paper, right? I do not think, we should have it in here. }
%Duplicate detection (also known as entity resolution) is the problem that finds different tuples that refer to the same real-world entity (e.g., iPad~Two = iPad~2). This problem has been extensively studied for several decades (see~\cite{DBLP:journals/tkde/ElmagarmidIV07} for a survey).  
%Most deduplication approaches consist of two phases: (1) \emph{Blocking}. 
%Due to the enormous cost of all pair comparisons, they first partition tuples into numbers of small blocks, and search for duplicates only within the same block. 
%For instance, if we partition \texttt{papers} based on \texttt{conference\_name}, then only the papers that are published in the same conference will be checked for duplicates; (2)\emph{Matching}. 
%To decide whether two tuples are duplicate or not, existing techniques typically model this problem as a classification problem, and train a classifier to label each tuple pair as duplicate or non-duplicate~\cite{DBLP:conf/kdd/BilenkoM03}. 
%Some recent works also explore the use of crowdsourcing and ask crowd workers to match tuple pairs~\cite{DBLP:journals/pvldb/WangKFF12,DBLP:conf/www/DemartiniDC12}.


%Although the blocking phase can be efficiently implemented using a hash index~\cite{journals/tkde/Christen11}, the matching phase is usually much more time-consuming, which either requires sophisticated machine-learning techniques or expensive human-labeling effort. %, thus the cost of deduplication process is typically measured by the number of pair comparisons in the matching phase~\cite{journals/tkde/Christen11}. 
%Consider a data size of $N$ and a sample size of $K$. Assume the average block size is $B$. Without sampling the matching phase has to compare $N\times B$ pairs of tuples. However, with the help of our \saqpplus framework the number is reduced to $K\times B$, which saves the matching cost by $\frac{N}{K}$ times. 


%We also compared the deduplication cost of our framework with the cost that required to find duplicates for the entire data. We found that our framework can save |DataSize|/|SampleSize| times of the number of pair comparisons. Note that this is usually the most costly step in the deduplication process, which either requires sophisticated machine-learning techniques or expensive human-labeling effort.



%\vspace{1em}
 

%\emph{W4-2. Are duplicates considered as many times as there are duplicates, i.e. every time when they show up in the relation? How does the duplication factor affect the number of tuples? Not at all?}

%[Ken:  again, this is unclear, as we don't know how many duplicates there truly are.]

%A2: Duplicates are considered as many times as there are duplicates. But it is worth noting that different representations of the same real-world entity are also considered as duplicates (e.g., $t_3$=``\texttt{YFilter Feb, 2002}" and $t_{10000}$=``\texttt{YFilter - ICDE 2002}" in Figure 1(a) of the paper). 

%The duplication factor does not affect the number of tuples. Thus, the notation ``$N$" in Table 1 represents the total size of dirty data (including duplicates).  We have added clarification in the title of Table 1.

%\tim{I did not find the explanation}


\mbox{}

\emph{W5. Section 6: I suggest to leave out Section 6 unless the authors provide a major rewrite and relate this section closer to the rest of the paper.}

A: We agree this section was not fully fleshed-out and requires more space to fully explore. In the revision we instead summarized its idea as part of future work and used the extra space for the other revisions and experiments we've added. % We will continue work on this in future work as it would be helpful to automatically manage the tradeoff between cleaning cost and answer quality.

%The original purpose of this section was to allow users to automatically manage the tradeoff between cleaning cost and answer quality. However, this problem turned out to be quite mathematically complex, requiring a convex optimization problem to allocate samples, and is less strongly tied to the rest of the contributions. Given the page limits, we decided to remove this section by following the reviewer's recommendation, and add its key highlights to the future work section. We dedicated the extra space to clarify and emphasize the practicality of our approach, and examples of how SampleClean can integrate with existing data-cleaning techniques.


\vspace{1em}


{\noindent \bf Response to the comments of Referee 2.}

\vspace{1em}

\emph{W1. This was a fun paper to read! The idea seems clever, but my concern is that it requires a very strong assumption - that you can clean a random sample of the data perfectly. The paper argues on page 2 that editing rules and master data enable reliable data repairs, but it's a bit of a stretch to assume that there is master data for every domain. I would argue that master data may be rare because it's difficult and costly to obtain, even more so for continuous attributes. And, while you could perhaps do a good job of cleaning some parts of the data, there is no guarantee that you can completely clean a random sample, as required in this paper.}




A: Thank you for pointing this out!  That was a strong assumption and as it turns out, an unnecessary one.  In the revision we relax the perfect cleaning assumption to explore imperfect cleaning and add a new experiment in Section~\ref{exp:sensitivity} that demonstrates that SampleClean works well even with imperfect cleaning.

% People use data cleaning to improve data quality, but it is usually expensive (costly or time consuming) to clean the entire dataset. Our SampleClean framework allows users to  clean only a sample of data with much less cost, and gives results that in expectation are as if applying the data cleaning to the entire dataset (called AllClean). With more samples cleaned, SampleClean will get closer and closer to AllClean, and finally converge to AllClean. Note that AllClean may not be the ``true value". But, if data cleaning leads to better data quality, AllClean will get closer to the ``true value". Accordingly, our SampleClean framework will give more accurate results as well.
In the experimental Section \ref{exp:sensitivity}, we show that when cleaning is not 100\% effective (some errors are not cleaned), SampleClean can still give better results compared to AllDirty (no cleaning but querying the entire dataset) for significantly lower cleaning costs (sampling 2000 out of 6M tuples with only 10\% effective cleaning).

%We compare our SampleClean framework to AllClean (cleaning the entire dataset) and AllDirty (no cleaning but querying the entire dataset). With SampleClean, as more tuples sampled the user will get closer and closer to AllClean. In general, AllClean may not be the "true value" or the "true value" may be impossible to know. However, our technique gives results that in expectation are as if applying the data cleaning to the entire dataset. 
%and it is upto the user to design an appropriate data cleaning module for their application and we provide an estimation framework around this module. 
%In the experimental Section 5.4, we show that even when cleaning is not 100\% effective (some errors are missed), this is still a useful technique to apply as we can give better results compared to AllDirty (no cleaning but querying the entire dataset) for significantly lower cleaning costs (sampling 2000 out of 6M tuples even with 90\% failure rate for the cleaning).

%\reminder{Add some related works before conclusion.}


\mbox{}

\emph{W2. Also, I would argue that data cleaning itself is an uncertain process and after data cleaning, you might end up with error bars on attribute values or multiple possible attribute values. In this case, it seems that the techniques proposed in this paper won't work.}

%[Ken: This is on deduplication, put in separate section?]

A: We agree that supporting multiple cleaned values can extend our framework to more data cleaning techniques, and have added this to our future work section. 

%We thank the reviewer for the suggestions. 
%Furthermore, we added new sections (2.3.1~and~2.3.2) that cite a variety of published cleaning methods, all of which provide certain cleaning results and can be applied with our framework.
While some data-cleaning techniques may provide multiple cleaned values, many existing data-cleaning techniques actually provide certain cleaning results, which can fit into our framework. For duplication errors, the following deduplication approaches can be applied to get a count of the number of duplicates for each tuple in the sample~\cite{elfeky2002tailor, conf/hdkm/Christen08, DBLP:conf/kdd/BilenkoM03, benjelloun2009swoosh}. 
%As mentioned in the previous response, these counts need not be perfect and our results are unbiased with respect to running respective deduplication tool over the entire dataset, which could be very expensive.  
For value and condition errors, we designed our framework with the intention of cleaning systematic errors (e.g., small attributes are missing from the database) rather than random errors (e.g., zero-mean Gaussian noise). 
Accordingly, the following recent works can address data cleaning problems of this form: filling missing values \cite{conf/icde/BethTMP13, parkcrowdfill} and fixing incorrect values~\cite{DBLP:journals/pvldb/YakoutENOI11, fan2012foundations,DBLP:conf/sigmod/DallachiesaEEEIOT13}. 



%and have added ``Uncertain Cleaning Results" into our future work section, and 
%We will investigate how to extend our framework to support uncertain cleaning values.

%We have added this in the future work section. 
%\tim{of what form? i am confused}


%We designed the input to our framework to match recent results in Data Cleaning, particularly from crowdsourcing. For duplication errors, the following entity-matching frameworks can be applied to get a count of the number of duplicates for each record in the sample \cite{elfeky2002tailor, tejada2001learning, benjelloun2009swoosh, thor2007moma}. 
%As mentioned in the previous response, these counts need not be perfect and our results are unbiased with respect to running respective entity-matching tool over the entire dataset, which could be very expensive.  
%For value and condition errors, we designed our system with the intention of cleaning systematic errors (e.g., small attributes are missing from the database) rather than random errors (e.g., 0 mean Gaussian noise). 
%Accordingly, the following recent works can address data cleaning problems of this form: filling missing values \cite{trushkowsky2013crowdsourced, parkcrowdfill, mayfield2009statistical} and fixing incorrect values \cite{mayfield2010eracer, yakout2010gdr, jeung2010end}.

%All of the cited works give single value results, even if imperfect. 
%The underlying problem is that without knowledge of the true value it is hard to quantify error bars around the estimate.
\iffalse
[Ken: I like this but either do the experiments or don't include this in the reply as it just opens another can of worms...]:

In future work, we will investigate how to extend our framework to support uncertain cleaning values. There are several ways to address this problem. For example, 
assume a data-cleaning technique provides ranges of cleaned values (e.g., a citation count is cleaned as a range between 100 and 120). We can run our result estimation approaches on the lower bound and the upper bound of the ranges, respectively, and take the union of the estimate results for a conservative estimate.
\fi
 


%We can run our result estimation on the lower bound and the upper bound and take the union of the ranges for a conservative estimate.
 

 
%In future work, we look to address the problem of quantifying uncertainty introduced by the data cleaning.
%These techniques can be amened to give probabilistic results, the analysis of which has been well studied works on probabilistic databases.
%For example, we can test our data cleaning module against other techniques and put approximate error bars on its estimate.
%In future work, we are also looking at supporting ranges of values (e.g., an attribute is between [20 40]).
%We can run our result estimation on the lower bound and the upper bound and take the union of the ranges for a conservative estimate.

%\reminder{List many references that do not give such uncertain cleaning results. Random error v.s. Systematical Error. Appreciate this hint. Discuss our idea that how to solve this problem. Defer to future work (Add it into the future work section).  }



\vspace{1em}

{\noindent \bf Response to the comments of Referee 3.}

\vspace{1em}


\emph{W1. It is mainly a statistical approach. When the results have a wide confidence interval, the answer is not satisfying.}

A: 
%
%Sampling provides a good trade-off between answer quality and query cost. By sampling more and more data, we can get better and better answer quality, i.e., narrower confidence interval. More formally, the size of the confidence interval is inversely proportional to the square root of sample size. 
%
In the revision we clarify that the size of the confidence interval is inversely proportional to the square root of sample size. 
For the queries in our experiments (Section 5), we found that even a small sample gave us sufficient accuracy. In Figure 11, we show that if we want to order the three authors by publication count in the MS Academic Search Dataset, we can clean only 276 records out of 1373 to achieve an ordering that is correct with 99\% probability. Likewise, in Figure 9, we show that cleaning 0.08\% of the data gives us results that are 95\% accurate. %Therefore, we argue that in many datasets and queries, we can achieve high quality results with small samples of data.

\mbox{}

\emph{W2. This combination of data cleaning, sampling and estimating about the whole data can only be implemented in aggregate queries (sum, avg, count...). }

A: That is true but is a limitation for all sampling-based approximate query processing approaches. Despite that, aggregate queries are widely used in OLAP applications, and being able to provide high-quality query results is crucial for decision making and strategy planing. We also provide support for aggregate queries with group-by clauses, allowing for queries whose results are more complex than single values.

\mbox{}


\emph{W3. typos: page 8, right column, 7.3, second paragraph "We can easily estimate..." : this sentence lacks of full stop
page 10, left column, "The results in Figure 10 show" (not showS) --}

A: Thank you: Fixed!

\mbox{}

\emph{W4. The authors' idea has a moderate novelty and is mainly based on statistics. Plus, they claim that "...this is the first work to marry data cleaning with sampling-based query processing", which does not address the relationship to SAQP (Sampling-based Approximation Query Processing).}


A:    We clarify in the revision, and added a subsection (Section~2.3.3, SampleClean v.s. SAQP):

\noindent \textit{{\noindent \bf \saqpplus v.s. \saqp:} \saqp assumes perfectly clean data while \saqpplus relaxes this assumption and makes cleaning feasible.
In \sampleclean, we take a sample of data, apply a data cleaning technique, and then estimate the result.
The result estimation is similar to \saqp, however, we require a few additional scaling factors related to the cleaning.
On the other hand, \bias is quite different from typical \saqp frameworks.
\bias estimates the average difference between the dirty and cleaned data, and this is only possible in systems that couple data cleaning and sampling. 
What is surprising about SampleClean is
that sampling a relatively small population of the overall data makes
it feasible to manually or algorithmically clean the sample, and
experiments confirm that this cleaning often more than compensates for
the error introduced by the sampling.}

%Both SAQP
%and SampleClean use sampling to estimate the value of aggregate
%queries.  SAQP assumes that data is perfectly clean: the error and
%associated confidences result from the sampling process.  SampleClean
%relaxes this assumption.  In SampleClean, error and associated
%confidence result from both dirtiness / inherent error in the data and
%from the sampling process.  What is surprising about SampleClean is
%that sampling a relatively small population of the overall data makes
%it feasible to manually or algorithmically clean the sample, and
%experiments confirm that this cleaning often more than compensates for
%the error introduced by the sampling.  

%We agree in retrospect that this was not made clear, and we clarify in the revised version.

% ignores errors in the database and gives results that may be biased due to these errors. SampleClean applies data cleaning to the sample and gives results that are in expectation as if the data cleaning was applied to the entire dataset. %While SAQP can never give results that are more accurate than a query of the entire dataset, when the sampling error in SampleClean is less than the pre-existing data errors, our result is more accurate than an aggregation of the entire dirty dataset.

\vspace{.5em}



\mbox{}

\emph{W5. The more important flaw is treating the cleansing as a black box without even providing a reference as to which technique it is that provides an output that is appropriate for the rest of the framework, nor discussing the sensitivity of the framework to the cleaning method.}


%A1: We added Section 2.3.1 to discuss how existing data-cleaning techniques can be applied in our framework. We also provided an end-to-end example implementation with data cleaning done by OpenRefine applied to our running example dataset of publications (See Example~1 in Section~2.3.2). This should make the problem that we are trying to solve more concrete and better characterizes the class of data cleaning techniques that are applicable to SampleClean.

A1: We added two new subsections (2.3.1 and~2.3.2) that cite a variety of published cleaning methods, all of which
can be applied with the SampleClean framework.


A2: We added a new experiment in Section \ref{exp:sensitivity} to evaluate the sensitivity of our framework to data cleaning. In summary, we showed that even if the data cleaning technique is only 10\% effective, SampleClean is still beneficial as we can get results that improve on AllDirty for a very small cleaning cost. 




%We added Section 2.3.1 to discuss how existing data-cleaning techniques can be applied in our framework. We also provided an end-to-end example implementation with data cleaning done by OpenRefine applied to our running example dataset of publications (See Example~1 in Section~2.3.2). This should make the problem that we are trying to solve more concrete and better characterizes the class of data cleaning techniques that are applicable to SampleClean.

%A: Thank you for pointing this out!  We have now addressed imperfect cleaning as noted above.  We added a new experiment in Section \ref{exp:sensitivity} to evaluate the sensitivity of our framework to data cleaning. In summary, we showed that even if the data cleaning technique is only 10\% effective, SampleClean is still beneficial as we can get results that improve on AllDirty for a very small cleaning cost. 



%\input{appendix.tex}

%\end{document}
