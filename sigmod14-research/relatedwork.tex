
\vspace{-1em}
\section{Related Work}\label{sec:rw}
{\noindent \bf Approximate Query Processing:} \saqp has been studied for more than two decades~\cite{DBLP:conf/vldb/GarofalakisG01,DBLP:journals/ftdb/CormodeGHJ12}.
Many \saqp approaches~\cite{DBLP:journals/tods/ChaudhuriDN07,DBLP:conf/eurosys/AgarwalMPMMS13,DBLP:conf/sigmod/AcharyaGPR99,DBLP:conf/cidr/SidirourgosKB11,DBLP:conf/sigmod/BabcockCD03,DBLP:conf/sigmod/HellersteinHW97,DBLP:journals/pvldb/PansareBJC11,DBLP:conf/sigmod/CondieCAHGTES10,DBLP:journals/pvldb/WuJOT09} were proposed, aiming to enable interactive query response times.
%Acharya et al.~\cite{DBLP:conf/sigmod/AcharyaGPR99} developed the Aqua system which is run on top of relational DBMS, and allows users to obtain approximate query answers in a fast response time. Chaudhuri et al.~\cite{DBLP:journals/tods/ChaudhuriDN07} studied how to create an optimal stratified random sample based on a given query workload, and proposed the STRAT algorithm that can make the workload queries achieve the best quality on the sample. Sidirourgos et al.~\cite{DBLP:conf/cidr/SidirourgosKB11} presented the SciBORQ architecture that creates biased multi-layered samples based on the scientific data exploration workload, and adaptively select a suitable sample to satisfy quality or time constraints during query execution. Agarwal et al.~\cite{DBLP:conf/eurosys/AgarwalMPMMS13} devised novel multi-dimensional stratified samples based on real-world big data analytics workloads, and built the BlinkDB system to utilize the created samples to support interactive query processing with error and response time constraints. Unlike the above approaches which create samples ahead of query time, online aggregation~\cite{DBLP:conf/sigmod/HellersteinHW97,DBLP:journals/pvldb/PansareBJC11,DBLP:conf/sigmod/CondieCAHGTES10,DBLP:journals/pvldb/WuJOT09} constructs samples in an online fashion, and gradually achieve more and more accurate results until users are satisfied about the current results. 
There are also many studies on creating other synopsis of the data, such as histograms or wavelets~\cite{DBLP:journals/ftdb/CormodeGHJ12}. While a substantial works on approximate query processing, these works mainly focus on how to deal with sampling errors, with little attention to data errors. 

\vspace{.5em}

{\noindent \bf Data Cleaning:} There have been many studies on various data-cleaning techniques, such as rule-based approaches~\cite{fan2012foundations,DBLP:conf/sigmod/DallachiesaEEEIOT13}, outlier detection~\cite{hellerstein2008quantitative,dasu2003exploratory}, filling missing values~\cite{conf/icde/BethTMP13, parkcrowdfill}, and duplicate detection~\cite{conf/hdkm/Christen08, DBLP:conf/kdd/BilenkoM03, conf/sigmod/WangLF12}. In order to ensure reliable cleaning results, most of these techniques require human involvement~\cite{DBLP:conf/sigmod/JefferyFH08,DBLP:journals/pvldb/FanLMTY10,DBLP:journals/pvldb/YakoutENOI11,DBLP:journals/pvldb/WangKFF12,DBLP:conf/sigmod/WangLKFF13}. 
%For example, Fan et al.~\cite{DBLP:journals/pvldb/FanLMTY10} proposed to employ editing rules, master data and user confirmation to clean data, and proved that their approaches can always lead to correct cleaning results. Wang et al.~\cite{DBLP:journals/pvldb/WangKFF12} proposed a hybrid human-machine approach to detect duplicate entities in data, which can achieve higher detection accuracy than machine-only approaches. 
In our paper, the main focus is not on a specific data-cleaning technique, but rather on a new framework that enables a flexible trade-off between data cleaning cost and result quality.
Indeed, we can apply any data-cleaning technique to clean the sample data, and then utilize our framework to estimate query results based on the cleaned sample. 


\vspace{.5em}

{\noindent \bf Result Estimation and Sampling:}
Estimating aggregate statistics of populations from samples has been well studied in the field of surveying \cite{weisberg2009total,valliant2000finite, hansen1987some, barnett1991sample, sarndal2003model, kalton1983introduction}.
These works explore different types of sampling, error characterizations, bias-variance trade-offs, and confidence intervals.
The theoretical foundation of surveying is statistical bounding of functions of independent random variables (e.g., samples with replacement).
This field includes distribution-free bounds such as Markov/Chebyshev/Hoeffding bounds, asymptotic bounds such as the Central Limit Theorem (CLT), and empirical testing such as Bootstrapping \cite{hinkley1988bootstrap}.
For a detailed survey of different types of statistical bounds refer to \cite{hahn2011statistical}.
Due to the CLT's strong guarantees (unbiased sample estimates and normalcy), as in our work, it is widely applied in the analysis of sampling schemes.
The more general study of sample estimators that are unbiased and have asymptotically normal distributions (like the CLT) is called U-Statistics, refer to \cite{lee1990u} for a survey of this theory.
The stochastic process literature also discusses problems in unbiased estimation from samples \cite{jacod1987limit}.

\vspace{.5em}

{\noindent \bf Distinct Value Estimation:}
Distinct value estimation has been an open problem in stream processing and database research~\cite{considine2004approximate,bar2002counting,haas1995sampling,beyer2007synopses}.
Similar to our analysis, some have argued that simply removing duplicates in a sample of data does not work~\cite{charikar2000towards}.
In the storage literature, similar weighted average techniques have been applied for global file duplication rate estimation \cite{harnik2012estimation} based on the knowledge of duplication rates within a sample.
This work can be seen as a simplified version of our problem only answering a count query with only duplication errors.
Techniques similar to our duplicate reweighting scheme have been studied in estimating from non-uniform samples \cite{aldroubi2002non}, and is also similar to the acceptance ratio used sampling algorithms such as the Metropolis-Hastings Algorithm and Rejection Sampling \cite{liu1996metropolized,metropolis1953equation}.
Further relevant work includes sensitivity analysis of set functions \cite{mcdiarmid1989method, jukna2012analysis} and statistical information theory \cite{kullback1997information}. 

