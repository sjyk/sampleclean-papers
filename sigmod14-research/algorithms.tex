\section{\bias Estimation}\label{sec:biascorrected}
\sampleclean estimates a result directly on a clean sample of data.
The size of confidence intervals in the \sampleclean estimate are function on the variance of
the cleaned sample $var(\saqpplusfunc(S))$ and the sample size $K$.
This property implies that the accuracy of \sampleclean is only dependent on the cleaned values (independent of the magnitude of incorrect values), and thus makes the technique robust to large errors.
This dependence may not be desirable in datasets where the data itself has high variance or where errors are small in magnitude.
%In this case, our desired estimation technique should give more accurate results for the data with small errors. 

This motivates the \bias approach, where we take an existing aggregation of the data, estimate its difference from the true value, and
then use the estimate to correct the existing aggregation.
The resulting technique gives us confidence intervals that are dependent on the variance of the errors, which can allow us to estimate very accurately on datasets
where this variance is small.
Furthermore, we provide the same unbiased guarantees on \bias as \sampleclean.

\subsection{Estimating the Difference}
Due to data errors, the result of the aggregation function~$f$ on the dirty population $P$ differs from the true result by~$\epsilon$:
\[f(P) = f(P_{\clean}) + \epsilon\]
In the previous section, we derived a function $\saqpplusfunc(\cdot)$ for \sampleclean estimation.
We contrasted this function with $\saqpfunc ( \cdot )$ which does not clean the data.
Therefore, we can write:
\begin{equation}
f(P) = \frac{1}{N}\sum_{t\in P}\saqpfunc(t)\hspace{2em}
f(P_{\clean}) = \frac{1}{N}\sum_{t\in P}\saqpplusfunc(t)
\end{equation}
If we solve for $\epsilon$, we find that:
\begin{equation}
\epsilon = \frac{1}{N}\sum_{t\in P}\Big(\saqpfunc(t)-\saqpplusfunc(t)\Big)
\end{equation}
In other words, for every tuple t, we calculate how much $\saqpplusfunc(t)$ changes $\saqpfunc(t)$.
For a sample $S$, we can construct the set of differences between the two functions:
\[\scriptsize
Q=\{\saqpfunc(t_1)-\saqpplusfunc(t_1),\saqpfunc(t_2)-\saqpplusfunc(t_2), \cdots ,\ \saqpfunc(t_K)-\saqpplusfunc(t_K)\}\]
The \mean difference is an unbiased estimate of $\epsilon$, the difference between $f(P)$ and $f(P_{\clean})$.
We can subtract this estimate from an existing aggregation of data to get an estimate of $f(P_{\clean})$.

\vspace{.5em}{\noindent \bf \bias Estimation}
We derive the \bias estimation procedure, which corrects an aggregation result:
\begin{enumerate}
\item Given a sample $S$ and an aggregation function $f(\cdot)$
\item Apply $\saqpfunc(\cdot)$ and $\saqpplusfunc(\cdot)$ to each $t_i \in S$ and call the set of differences  $Q(S)$.
%Set $s_i$ to zero if the tuple does not satisfy the predicate in the dirty data.
\item Calculate the mean $\mu_q$, and the variance $\sigma_q$ of $Q(S)$
\item Return $(f(P) - \mu_q) \pm \lambda \sqrt{\frac{\sigma_q^2}{K}}$
\end{enumerate}

Similar to \sampleclean, we can prove that the result in unbiased:
\begin{theorem}\label{thm:bias}
Given an aggregation function $f$ and a population $P$,
where there are three types of errors: value, condition, and duplication.
Let S be a uniform sample $S \subseteq P$ of size $K$.
Let $Q$ be the set of tuples where $q(t)=\saqpfunc(t)-\saqpplusfunc(t)$ is applied to every tuple.
Then the estimate on this sample is given by:
\[ mean(Q) = \frac{1}{K}\sum_{t\in S} \Big (\saqpfunc(t)-\saqpplusfunc(t) \Big)\]
is an unbiased estimate of the bias $\epsilon$.
It follows, that $f(P)-\epsilon$ is an unbiased estimate of the result.
\end{theorem}
\begin{proof}[sketch] We can apply the theory developed for \sampleclean to the estimate of $\epsilon$.
Since it is unbiased, due to the linearity of expectation the estimate $f(P)-\epsilon$ is also unbiased.
See~\cite{saqpfull} for a detailed proof.
\end{proof}

Consider the Example~\ref{exa:sampleclean} discussed in the previous section.
\begin{example}
Based on the definition of $\saqpfunc(\cdot)$ in Section~\ref{subsec:resultestimation}, we have 
\[\saqpfunc(S) = \{0,\ 3661,\ 0,\ 0,\ 250,\ 2,\ 0\}. \]
For example, as shown in Figure~\ref{fig:sample}, since $t_1$ does not satisfy the predicate, we have $\saqpfunc(t_1) = 0$.
Additionally, as $t_2$ satisfies the predicate, and its dirty citation is 1569 and the scaling for predicates is 
$\frac{K}{K_{\pred}}=\frac{7}{3}$, we have $\saqpfunc(t_2) = \frac{7}{3}\cdot 1569 = 3661$. 

Over the same data we apply $\saqpplusfunc(\cdot)$ (as shown in Example~\ref{exa:sampleclean})  
\[\saqpplusfunc(S) = \{133,\ 2895,\ 275,\ 0,\ 197,\ 63,\ 0\}, \]
we can obtain the difference between the two samples
\[Q(S) = \{-133,\ 766,\ -275,\ 0,\ 53,\ -61,\ 0\}.\]
We calculate the \mean $\mu_q$ and the variance $\sigma_q^2$ of $Q(S)$, and return $(f(P) - \mu_q) \pm \lambda \sqrt{\frac{\sigma_q^2}{K}}$.
\end{example}

\subsection{\bias vs. \sampleclean}
We compare \sampleclean and \bias on result accuracy and processing time.
Both methods achieve unbiased estimates, but may differ greatly in the accuracy (the size of the confidence interval) of these estimates.
The confidence interval of \bias is given by $\pm\lambda\sqrt{\frac{\sigma_q^2}{K}}$ and for \sampleclean it is  $\pm\lambda\sqrt{\frac{\sigma_c^2}{K}}$.
Therefore, for a fixed sample size, if $\sigma_c \ge \sigma_q$, \bias will be more accurate.
In cases when either $\sigma_c$ is large or $\sigma_q$ is small, \bias can give a result with narrower confidence intervals than \sampleclean.
For example, if we had no data errors then $\sigma_q = 0$ (a perfect \bias estimate), but $\sigma_c$ would still be non-zero as it is the variance of the cleaned data.
Conversely, the more unpredictable (high variance in the difference between the clean and dirty data) the error offset is, the worse \bias will perform.

In terms of processing time, \bias is very different from \sampleclean as it needs to run an aggregation query on the entire dataset.
We highlight two situations that are not affected by this additional processing.
(1) When the time required to clean a sample is much larger than the time needed to scan the entire dataset.
(2) When the user has an existing result on the dirty dataset and wants to assess how far away it is from the true value.
In these settings, we can maintain results for both \sampleclean and \bias and return the result with a tighter confidence interval.

The analysis in this section assumes that we can compute an exact aggregation result on the dirty dataset.
We can also extend \biascorrected by estimating the aggregate result based on a sample without accessing the entire data.
In this way, \biascorrected requires less response time but may lose some result accuracy.
Interested readers are referred to~\cite{saqpfull} for details.

%\subsection{Discussion}
%While \sampleclean and \biascorrected are functionally similar, they may differ greatly in the accuracy of their estimates.
%In \bias, the size interval is determined by the variance of the set Q, i.e., $\sigma_q$, not $\sigma_c$ as in \sampleclean.
%In cases when either $\sigma_c$ is large or $\sigma_q$ is small, \bias can give a result with narrower confidence intervals than \sampleclean.
%Conversely, the more unpredictable (high variance in the difference between the clean and dirty data) the error offset is, the worse \bias will perform.
%After already spending the cleaning effort, it is relatively cheap to maintain results for both \biascorrected and \sampleclean, and we can simply report the one with a narrower confidence interval.
%However, as discussed earlier, \bias is only feasible when we can get an exact aggregation result on the dirty data.
%It may not be suited for ad-hoc or interactive applications.
%{\noindent \bf Remark.} When data becomes larger, it might be costly for \biascorrected to obtain an exact aggregate result of the entire dirty data.
 







