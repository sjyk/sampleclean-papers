\begin{abstract}
%Aggregate query processing on very large datasets (Big Data) can be slow and prone to error due to dirty (missing, erroneous, duplicated, or corrupted) values.  It is well-known that if data is cleaned, sampling is very effective for increasing speed.  In this paper we show that sampling can be integrated with data cleaning to achieve both speed and accuracy.  Cleaning data requires either domain-specific software (which can be costly and time-consuming to develop) or human inspection.  The latter is increasingly feasible with crowdsourcing but highly inefficient for large datasets.  We
%present \saqpplus, a sample-and-clean framework that requires cleaning only samples from the dataset to process aggregate queries (eg. avg, sum, count, etc).  We prove \saqpplus can obtain unbiased query results, and also derive confidence intervals as a function of sample size to bound their result errors.  We also apply allocation theory to optimize sample sets for group-by queries.
%We evaluate \saqpplus with three experiments: 1) adding synthetic noise to 6M tuples from the TPC-H shipping database benchmark, 2) manually cleaning 1373 tuples from a Microsoft citation index, and 3) using a published algorithm to clean 44,460 indoor sensor network data values.
%Results suggest that estimated values can rapidly converge toward the true values with surprisingly few samples, offering significant improvement in speed over cleaning all of the data and significant improvement in accuracy over cleaning none of the data.



Aggregate query processing over very large datasets can be
slow and prone to error due to dirty (missing, erroneous, duplicated,
or corrupted) values.  To address the speed issue, there has lately
been a resurgence of interest in sampling-based approximate query processing,
but this approach further reduces answer quality by introducing sampling error.
In this paper we explore an intriguing opportunity that sampling
presents, namely, that when integrated with data cleaning, sampling
actually improves answer quality. Data cleaning requires either
domain-specific software (which can be costly and time-consuming to
develop) or human inspection.  The latter is increasingly feasible
with crowdsourcing but can be highly inefficient for large datasets.
Our approach requires cleaning only samples from the
dataset to process aggregate numerical queries such as average, sum,
count, variance, geometric mean, and product.  We derive confidence intervals as
a function of sample size and show how our approach reduces error bias.  
%We also apply allocation theory to optimize cleaning cost and answer quality for group-by queries.
We provide a detailed evaluation of our approach using datasets and workloads from
the \mbox{TPC-H} benchmark, an on-line citation index and a database of indoor sensor
network values.  Our results suggest that estimated values can rapidly converge
toward the true values with surprisingly few cleaned samples, offering
significant improvement in cost over cleaning all of the data and
significant improvement in accuracy over cleaning none of the data.



%When querying big data, there are typically two sources of error in the query result: sampling error and data error. 
%To deal with the big volume of data, people usually create a random sample of the data, and run queries on the sample to obtain approximate results. However, in practice, result quality is not only dependent on sampling errors since the data itself may contain errors.
%Due to these data errors, even queries on the entire data may still lead to arbitrarily erroneous results. Although data cleaning can fix the data errors, it is a very costly process, and hard to scale to big data.

%Sampling-based approximate query processing (\saqp) is a powerful technique to manage sampling errors. 

%A key feature of \saqp is a flexible trade-off between result quality and query response time, and a user can sample as much or as few data to meet desired quality or time constraints.
 
%Sampling is widely used in improving query response times. In this paper, we explore how to use sampling to improve result quality. We propose \saqpplus, a scalable data-error management framework, which allows users to clean a sample of data, and utilizes the cleaned sample to estimate aggregate query results.
%\saqpplus consists of two estimation approaches: (1) \biascorrected uses the cleaned sample to estimate how data errors affect query results, and applies it to correct the erroneous aggregate results of the uncleaned data; (2) \sampleclean uses the cleaned sample to directly estimate the accurate results of the cleaned data. As such, \saqpplus is able to provide a flexible trade-off between data-cleaning cost and result quality.
%We provide analysis to show that these approaches give unbiased estimates with quality guarantees, and we can adaptively meet user-specified quality or cleaning cost constraints. 
%Our experiments on real and synthetic data sets suggest that \saqpplus improves result quality for a variety of different queries and data errors.
%Even more surprisingly, in some experiments, aggregations on only samples (i.e. \sampleclean) achieved more accurate results than aggregations of the entire data.
%Its basic idea is to create a random sample of the data, and then run aggregation queries on the sample to obtain approximate results along with the quality guarantees. While sampling can save the query time, people usually prefer to obtaining higher accurate results from more data. However, they ignore the fact that the real-world data may have errors.  Even running a query over the entire data may still not get the accurate result. Although data cleaning can help to fix the data error, it is a very costly process, and hard to scale to large data. To address these issues, we propose an improved \saqp framework in our paper, called \saqpplus, which cleans a sample of the data, and then estimates query results based on the cleaned sample. We develop two estimation methods in our framework: (1) \sampleclean directly estimates the query result based on the cleaned sample; (2) \biascorrected first estimates the data error based on the cleaned sample, and then uses it to correct the query result of the entire data. We also investigate how to adaptively create samples to meet user-specified cleaning-cost or result-quality constraints. Experimental results on both real and synthetic data indicates that, by only cleaning a small sample of the data, \saqpplus can achieve very accurate query results, and even better than those computed from the entire data.
\end{abstract}