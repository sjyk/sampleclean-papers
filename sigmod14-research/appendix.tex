\clearpage
\newpage
\section{Appendix}
The appendix is organized in the following way.
In Section \ref{app:proof}, we prove the unbiasedness of our estimation.
In Section \ref{app:ext}, we discuss how \sampleclean and \bias can be extended to different functions, including \varfunc.
Finally in Section \ref{app:ext2}, we discuss how \bias can be extended to make unbiased estimates from existing sample aggregations.

\subsection{Proofs}\label{app:proof}
In this section, we detail the theorems and the lemmas used in the paper.
Our main goal is to show that our estimates are unbiased; that is in expectation (denoted by $\mathbb{E(.)}$) they give the right answer.
Throughout these proofs, we will apply some key properties of expected values, for random variables $x$ and $y$:
\begin{itemize}
\item Linearity: $\mathbb{E}(ax+by)=a\mathbb{E}(x)+b\mathbb{E}(y)$
\item Independence: if $x$ and $y$ are independent then \\$\mathbb{E}(f(x)g(y))=\mathbb{E}(f(x))\mathbb{E}(g(y))$
\item Measurable functions: $\mathbb{E}(f(x))= \int f(x) \mathbb{P}(x)$ or in the discrete case $\mathbb{E}(f(x))= \sum f(x) \mathbb{P}(x)$
\end{itemize}

\subsubsection{Unbiased Estimates From Reweighted Data}
We proposed a reweighting technique to account for the over-representation of duplicates in samples.
To show that this gives us an unbiased estimate, our proof strategy will be to prove results on the full dataset, and then extend the results to a sample.

{\noindent \bf \avgfunc query:}
We begin with the following basic lemma about reweighting the entire dataset and how a mean of the reweighted data differs from the true mean.

\setcounter{lemma}{0}
\begin{lemma}
Let $P$ be a population with $N$ data tuples, and $P_{u}$ be the cleaned population with $N'$ unique data tuples.
For each $p_{i}\in P$, let $m_i$ denote the number of its duplicates in $P$.
Then mean value of the set $\saqpplusfunc(P)=\{\frac{p_{1}}{m_{1}},\frac{p_{2}}{m_{2}},...,\frac{p_{N}}{m_{N}}\}$ is:
\[
mean(\frac{N}{N'}\saqpplusfunc(P))= mean(\hat{P}_{u})
\]
\end{lemma}

Proof: The function $\saqpplusfunc$ acts point-wise on the elements of the dirty set $P$, creating a new set $\saqpplusfunc(P)$.
Therefore:
\[
mean(\saqpplusfunc(P)) = \frac{1}{N}\sum_i^N \frac{x_i}{m_i}
\]
We collect all the terms with the same value; that is a set ${i:x_i=z}$.
We call the set of distinct values the support, denoted by $supp(p)$.
\[
mean(\saqpplusfunc(P)) = \frac{1}{N}\sum_{z \in supp(p)} z(\sum_i \frac{1}{m_i})
\]
If we mutliply each $z$ by $\frac{N'}{N}$, then:
\[
mean(\saqpplusfunc(P)) = \sum_{z \in supp(p)} z\frac{(\sum_i \frac{1}{m_i})}{N'}
\]

The aggregation attributes of a population of numerical tuples can be interpreted as a discrete probability distribution.
For example, in the set $\{1, 1, 3\}$, we can say that an equivalent representation is a distribution $p(x)$, where $p(1)=\frac{2}{3}$,
and $p(3)=\frac{1}{3}$.
This probability distribution is exactly the probablity that a value will be drawn in a uniform random sampling.
Furthermore, let us say that $p(x)$ is a representation for the clean population $P_{u}$.
The expected value of this distribution is consistent with the mean of the set:
\[
\mathbb{E}(p(x)) = \sum_z z \mathbb{P}(x=z)
\]
\[
\mathbb{E}(p(x)) = mean(P_{u})
\]
and furthermore $\mathbb{P}(x=z)=\frac{(\sum_i \frac{1}{m_i})}{N'}$:
\[
mean(\frac{N}{N'}\saqpplusfunc(P))= mean(\hat{P}_{u})\blacksquare
\]

We use this lemma to prove that our estimates on a sample are unbiased.
This was presented as Lemma \ref{lem:derror} in paper.
Specifically, we mean that in expectation the mean of $\saqpplusfunc(S)$ is equal to result on the full cleaned data $\saqpplusfunc(P)$.
\begin{lemma}
Let $S \subseteq P$ be a sample of a population with $K$ data tuples, and $P_{u}$ be the cleaned population as defined before.
For each $s_{i}\in S$, let $m_i$ denote the number of its duplicates in $P$.
Then expected mean value of the set $\saqpplusfunc(S)=\{\frac{s_{1}}{m_{1}},\frac{s_{2}}{m_{2}},...,\frac{s_{K}}{m_{K}}\}$ is:
\[
\mathbb{E}(\frac{K}{K'}mean(\saqpplusfunc(S))) = mean(P_{u})
\]
where $K'=\sum_i\frac{1}{m_i}$.
\end{lemma}
Proof: A caveat is that $m_i$ is also a random variable.
Since, we assume uniform random sampling, we can conclude that each $s_i,m_i$ are independent from other samples:
\[
s_{i} \perp s{j}\text{, } m_{i} \perp m{j}
\]
Using this intuition, we want to isolate a sample and its duplication number.
Let us first consider a single sample $s_{i}$ from the mean calculation,
we hold out the terms relating to $i$ and express it as:
\[
\frac{K}{\frac{1}{m_i}+ \sum_{j\ne i}\frac{1}{m_j}} \frac{s_i}{m_i}
\]
With a little bit of algebraic manipulation, we can get:
\[
\frac{K(\sum_{j\ne i}\frac{1}{m_j})}{(K-1)\sum_i\frac{1}{m_i}}\cdot\frac{K-1}{\sum_{j\ne i}\frac{1}{m_j}}\cdot\frac{s_i}{m_i}
\]
Now, if we take expectations,
\[
\mathbb{E}(\frac{K(\sum_{j\ne i}\frac{1}{m_j})}{(K-1)\sum_i\frac{1}{m_i}}\frac{K-1}{\sum_{j\ne i}\frac{1}{m_j}}\frac{s_i}{m_i})
\]
apply linearity, and notice the symmetry in the problem (if we sample independently each samples is probabilistically the ``same");
we can see that by the first term will be 1 in expectation.
In other words, since we are holding out 1 item each time, which means the average will be exactly $\frac{K}{K-1}$.
We are left with:
\[
\mathbb{E}(\frac{K-1}{\sum_{j\ne i}\frac{1}{m_j}}\frac{s_i}{m_i})
\]
Applying independence to the rest of the expression, this is equal to:
\[
\mathbb{E}(\frac{K-1}{\sum_{j\ne i}\frac{1}{m_j}})\mathbb{E}(\frac{s_i}{m_i})
\]
The first term in an independent unbiased estimate of $\frac{N}{N'}$ and the second term is mean of $mean(\saqpplusfunc(P))$:
\[
\frac{N}{N'}\cdot mean(\saqpplusfunc(P)) = mean(P_{u})\blacksquare
\]

{\noindent \bf \sumfunc and \countfunc queries:}
These queries follow from the same logic as the \avgfunc query, but with a much simplified proof for both lemmas.
As we do not need to deal with a scaling factor such as $\frac{N}{N'}$, the first theorem is trivially shown by linearity of expectation.
In the second theorem, since we can treat $\frac{s_i}{m_i}$ as a single random variable (there is no dependence of $K'$ on $m_i$) so each sample
is trivially independent.
We can then use the fact that sample means are unbiased estimators of population means.

\subsubsection{Unbiased Estimates With Predicates}\label{app:proof2}
A similar proof is needed to show that estimates with predicates are unbiased.

{\noindent \bf \avgfunc queries:}
We will explore how predicates affect the \avgfunc query.
\begin{lemma}
Let $S \subseteq P$ be a sample of a population with $K$ data tuples, and $P_{1}\subset P$ be the set of tuples that satisfy the predicate.
For each $s_{j}\in S$, let $I(j)$ be an indicator function of whether the tuple satisfies the predicate or not.
Then expected mean value of the set $\saqpplusfunc(S)=\{I(1)s_1,I(2)s_2,...,I(K)s_K \}$ is:
\[
\mathbb{E}(\frac{1}{r}mean(\saqpplusfunc(S)))= mean(P_{1})
\]
where $r = \frac{\sum_j I(j)}{K}$.
\end{lemma}
Proof: We first can expand the expression, noticing that the factors of $K$ cancel out:
\[\frac{1}{r}mean(\saqpplusfunc(S)) = \frac{1}{\sum_j I(j)}\sum_n I(n)s_n \]
Since only the non-zero elements contribute to the sum, then this is the same as the mean over the elements in
the sample that satisfy the predicate.
Then, it follows that, in expectation, each sample that satisfies the predicate has mean:
\[ mean(P_{1})\]
Applying the linearity of expectation:
\[
\mathbb{E}(\frac{1}{r}mean(\saqpplusfunc(S)))= mean(P_{1})\blacksquare
\]

{\noindent \bf \sumfunc and \countfunc queries:}
Similar to duplication error, as we do not have to worry about the scaling factor $r$ for these quries the proof is greatly simplified.
In fact, as before it directly follows from the linearity of expectation.



\subsubsection{Proof of Theorem~\ref{thm:sampleclean}}\label{app:proof3}
The lemmas in the previous sections for duplication errors and predicates are careful to prove unbiased estimates only under their respective errors.
However, the theorems are general enough not to make any assumptions about the presence of other errors.
We can compose these corrections in the intuitive way; where we first set the attribute to its correct value, reweight the value by $m_i$, and set it to $0$ if it does not satisfy the predicate.

The proof is clear, since we are taking one pass making the estimate unbiased under value errors.
Then in another pass our estimate is unbiased under duplication.
Finally, we transform the set so its mean in unbiased with the predicate.

To be pedantic, the order of operations is important since our transformation sets condition errors to $0$.
Therefore, it should be the last step in the composition.
Duplication and value correction are interchangeable.

\subsubsection{Proof of Theorem~\ref{thm:bias}}\label{app:proof4}
Based on the proofs in the preceeding appendix sections, we can see how \sampleclean gives unbiased estimates.
In \sampleclean, we take a sample of data, clean it by applying the function $\saqpplusfunc$, and then aggregate the resulting set with a mean.
It directly follows from the two lemmas that for \sumfunc, \countfunc, and \avgfunc, we can achieve an unbiased estimate.
We presented this as Theorem \ref{thm:sampleclean} in the paper.

We also argued in Theorem \ref{thm:bias} that \bias gives an unbiased estimate.
The result of the aggregation function $f$ on the dirty population $P$ differs from the true result by a bias $\epsilon$:
\[f(P) = f(P_{\clean}) + \epsilon\]
We can write:
\begin{equation}
f(P) = \frac{1}{N}\sum_{t\in P}\saqpfunc(t)\hspace{2em}
f(P_{\clean}) = \frac{1}{N}\sum_{t\in P}\saqpplusfunc(t)
\end{equation}
If we solve for $\epsilon$, we find that:
\begin{equation}
\epsilon = \frac{1}{N}\sum_{t\in P}\Big(\saqpfunc(t)-\saqpplusfunc(t)\Big)
\end{equation}
In other words, for every tuple t, we calculate how much $\saqpplusfunc(t)$ changes $\saqpfunc(t)$.
For the population $P$, we can construct the set of differences between the two functions:
\[\scriptsize
Q=\{\saqpfunc(t_1)-\saqpplusfunc(t_1),\saqpfunc(t_2)-\saqpplusfunc(t_2), \cdots ,\ \saqpfunc(t_N)-\saqpplusfunc(t_N)\}
\]
The mean of this set is:
\[
\epsilon = mean(Q)
\]
Similarly, for a sample $S$, we can take a subset of $Q$:
\[\scriptsize
Q_s=\{\saqpfunc(t_1)-\saqpplusfunc(t_1),\saqpfunc(t_2)-\saqpplusfunc(t_2), \cdots ,\ \saqpfunc(t_K)-\saqpplusfunc(t_K)\}
\]
If we take the expected mean value of this set:
\[
\mathbb{E}(mean(Q_s))=\mathbb{E}(\frac{1}{K}\sum_i\saqpfunc(t_i)-\saqpplusfunc(t_i))
\]
By linearity of expectation and indpendence, we can see:
\[
\mathbb{E}(mean(Q_s))=\mathbb{E}(\saqpfunc(t_i))-\mathbb{E}(\saqpplusfunc(t_i))
\]
Using the arguments developed for sample clean:
\[
\mathbb{E}(\saqpplusfunc(t_i)) = f(P_{\clean}) 
\]
\[
\mathbb{E}(\saqpfunc(t_i)) = f(P) 
\]
Finally,
\[
\mathbb{E}(mean(Q_s))=\epsilon
\]
By linearity of expectation, if we subtract this unbiased estimate of $\epsilon$
from an existing aggregation of data, we get an unbiased estimate of $f(P_{\clean})$ $\blacksquare$.

\subsubsection{Estimate Monotonicity and Convergence}
We can also show convergence of our estimates.
\begin{theorem}
Let $r_{d}$ be the result of an aggregation of the dirty data
with an absolute error of $e_{d}$. For aggregations of the form that
we proposed, there exists a $K$ such that for all $k\ge K$ samples
cleaned the resulting estimate will always be have an error $\hat{e}(k)\le e_{d}$.
Furthermore, the sequence $\hat{e}(k),\hat{e}(k+1),....,\hat{e}(N)$
is strictly decreasing.
\end{theorem}

Proof: The proof is clear, but it is an important conceptual point
about our estimation framework. We showed in Section \ref{sec:sampleclean} that the
95\% confidence interval for \sampleclean was of the form $\pm\lambda\frac{\sigma_{c}}{\sqrt{k}}$
and \bias was of the form $\pm\lambda\frac{\sigma_{q}}{\sqrt{k}}$.
We omitted the finite population correction factor from these confidence intervals:
\[ FPC = \frac{\sqrt{N-k}}{\sqrt{N-1}}\]
So the confidence intervals should be of the form:
\[\pm FPC\lambda\frac{\sigma}{\sqrt{k}} \]
This function is strictly decreasing to 0, and therefore there exists a such that all $k\ge K$ result in an estimate bounded by the original data error.
We are not only unbiased but we are guaranteed to converge to the right result.

\subsubsection{Narrower Confidence Intervals}
The confidence intervals for the \avgfunc query presented in the paper give a conservative estimate.
This is because $r$ itself is a random variable with some additional variance.
However, our choice of $r$ leads to some cancellations actually help us get a more accurate estimate.

Consider the following analysis.
If we set the attributes that don't satisfy the predicate to $0$, it forms a mixture distribution whose variance is given by:
\[ \frac{1}{r^2}\frac{r\sigma^2+(1-r)r\mu^2}{K} \]
Or simplified:
\[\frac{\sigma^2+(1-r)\mu^2}{Kr} \]
However, in our proof, we treated $r$ as skipping the tuples do not satisfy the predicate.
If we consider the variance of the skipping interpretation, it is:
\[ \frac{\sigma^2}{rK}\]
We see that there is an additional dependence on $\mu$ in the error bars presented in the paper, but we can achieve strictly smaller error bars.

\subsection{Extensions to result estimation model}\label{app:ext}
In this section, we discuss extensions to our result estimation model.

\subsubsection{Variance and Standard Deviation}
The underlying reason why the variance function is challenging to estimate is that in general $var(X+Y) \ne var(X)+var(Y)$.
With this in mind, we will first consider the simpler case of \sampleclean.
We define the sample variance of a set of real numbers $X$ as:
\[ var(X) = \mathbb{S}(X^2)-\mathbb{S}(X)^2 \]
where $X^2$ denotes the set $X^2=\{x_1^2,x_2^2,...,x_k^2\}$, and where $\mathbb{S}(.)$ denote the sample mean of a set of numbers.
Both the quantities $\mathbb{S}(X^2)$ and $\mathbb{S}(X)^2$ can be easily estimated with our framework.
If we define a set $X^2=\{x_1^2,x_2^2,...,x_N^2\}$, we can simply apply our estimation framework to estimate a mean of that set to get $\mathbb{S}(X^2)$.
Similarly, we can simply square estimate the mean of $X$ to get $\mathbb{S}(X)^2$.
Subtracting the two values gives us an unbiased result\footnote{We omit a correction factor of $\frac{N}{N-1}$ which makes the sample variance an unbiased estimate of the population variance.}.

The problem is that while we have computed a result that is correct in expected value, calculating a confidence interval is more challenging.
Since $\mathbb{S}(X^2)$ and $\mathbb{S}(X)^2$ are not independent estimates, we cannot simply add their confidence intervals; nor can we square the intervals in $\mathbb{S}(X)^2$.
Our solution is to use a statistical bootstrap to calculate an empirical confidence interval around the estimate.

\bias follows from the same reasoning but requires a little bit more algebraic manipulation.
As defined \bias relies on the additivity of the mean function \[mean(X-Q)=mean(X)-mean(Q)\] however, in general \[var(X-Q)\ne var(X) - var(Q)\]
Recall, that $X$ represents the set of dirty tuples $\{x_1,x_2,...,x_N\}$ and $Q$ is the set of
differences between the dirty and clean data: \\$\{\saqpfunc(x_1)-\saqpplusfunc(x_1),\saqpfunc(x_2)-\saqpplusfunc(x_2),...,\saqpfunc(x_N)-\saqpplusfunc(x_N)\}$.
It turns out that variance of $var(X-Q)$ is dependent on a more complicated expression with a cross-term $cov(X,Q)$:
\[ var(X-Q) = var(X)+var(Q)-2cov(X,Q) \]
We have to estimate two quantities $var(Q)$ and $cov(X,Q)$ which we can calculate using a similar technique as in \sampleclean:
\[ var(Q) = \mathbb{S}(Q^2)-\mathbb{S}(Q)^2 \]
\[ cov(X,Q) = \mathbb{S}(XQ)-\mathbb{S}(X)\mathbb{S}(Q) \]
Where $XQ$ denotes a point-wise multiplication of the sets $X$ and $Q$.
Our method can support such forms as $\mathbb{S}(XQ)$, by materializing a single column representing \\$XQ={x_1q_1,x_2q_2,...,x_Nq_N}$.
Similar to \sampleclean, we use a statistical bootstrap to ascertain our confidence about this estimate. 

{\noindent \bf Bootstrapping:}
Bootstrapping is a well studied field in statistics \cite{hinkley1988bootstrap}, and there are many variants of the same general principle.
If we have an aggregation function $f$ and a set $X$, we apply $f$ to re-sampled versions $X$: $\{f(X_1),f(X_2),...\}$.
We iteratively build a distribution of results and can use this to empirically figure out a confidence interval (eg. 95\% of the results lie between these estimates).
The intuition behind the method is that we are empirically evaluating the sensitivity of $f$ to sampling.
For our task, a particularly Bootstrapping variant is the Bag-of-little-Bootstraps \cite{kleiner2011scalable}, which allows for convient subsampling without replacement, but in principle we can apply any technique.

\subsubsection{Geometric Mean and Product}
Another simple extension of the framework is to calculate the multiplicative analogs of \avgfunc and \sumfunc:
\begin{itemize}\vspace{-.5em}
\item $\geomeanfunc(X) = \sqrt[N]{\prod_{i=1}^Nx_i}$
\item $\productfunc(X) = \prod_{i=1}^Nx_i$
\end{itemize}
The  are only meaningful for strictly positive values, so assuming strictly positive values we can take a log of the terms $log(gm(X))$ and $log(p(X))$, and then this
problem becomes a familiar sample mean form:
\begin{itemize}\vspace{-.5em}
\item $\log(\geomeanfunc(X)) = \frac{1}{N}{\sum_{i=1}^N\log(x_i)}$
\item $\log(\productfunc(X)) = {\sum_{i=1}^N\log(x_i)}$
\end{itemize}
We can solve this sample mean form with the methods/parameters described in the earlier sections, taking an exponential of the result and the confidence intervals at the end
when reporting the results.
An example application of this is calculating conditional probabilities in a Naive Bayes classifier \cite{jordan2002discriminative}.

\subsection{\bias for a Sample Aggregation}\label{app:ext2}
We can extend the \bias algorithm to work with aggregations of samples of the dirty data.
Naturally, this will trade-off computation time for estimate variance.
This extension affects the confidence interval in a very intuitive way.
If $S_1$ is sampled independently of $S_2$, then it follows that their estimates are independent.
The variance of two independent variables is the sum of the variances.
We can calculate the variance of the aggregation on the dirty sample, and then add it to the variance of the bias estimate $\frac{\sigma_q^2}{k}$. The following shows the details of the estimation method:

\begin{enumerate}
\item Given two random samples $S_1$ and $S_2$ and an aggregation function $f(.)$
\item Apply the $\saqpfunc(.)$ to each $s_i \in S_1$ and call the result $Q(S)=\{\saqpfunc(s_1)-\saqpplusfunc(s_1),\saqpfunc(s_2)-\saqpplusfunc(s_2),...,\saqpfunc(s_k)-\saqpplusfunc(s_k)\}$
\item Calculate the mean $\hat{\mu}_q$, and the variance $\hat{\sigma}_q$ of $Q(S_1)$.
\item Calculate the variance of the aggregation on the dirty data $\sigma_d^2=var(\saqpfunc(S_2))$ 
\item Return $(f(S_2) - \hat{\mu}_q) \pm \lambda \sqrt{\frac{\hat{\sigma}_q^2}{|S_1|} + \frac{\hat{\sigma}_d^2}{|S_2|}}$.
\end{enumerate}

This does complicate the problem of duplicate estimation, since if we cannot aggregate over the entire dataset, we cannot search over the entire dataset for duplicates.
Suppose, we were restricted to duplicate search within the sample $S_2$, then we would consistently underestimate the number of duplicates for each tuple.
We develop the following theory about estimates with imperfect duplicate counts.
Intuitively, we claim that conservative duplicate correction, that is underestimating duplicates and doing this correctly/consistently, is better than no duplicate correction.

\begin{theorem}
Define $\saqpplusfunc^{1}$ be a data transformation that only corrects for false positive errors and aggregation errors.
As it does not correct for duplication errors, there will be a bias $e$, the difference between the true result and estimate:
\[ e = |f(P_{clean})- mean(\saqpplusfunc^{1}(p))|. \]
Furthermore, let $\saqpplusfunc^{2}$ be a data transformation that corrects for all three errors but may underestimate the number of duplicates for each tuples;
that is $m'_{i}\le m_{i}$.
Finally, let $\saqpplusfunc$ the data transformation that cleans correctly for all errors.
We insist that these duplicate estimates in (2) $m_{i}'$ are consistent (ie. symmetric); if tuple i is a duplicate of tuple j, the tuple j is a duplicate of tuple i.
Let $b$ be the bias of an estimate using this transformation:
\[ b = |f(P_{clean})-d_2 \cdot mean(\saqpplusfunc^{2}(p)|. \]
Then, we have the following bounds:
\begin{equation}
0 \le b \le e
\end{equation}
Where $0\le b$ is tight when $m_i'=m_i$ and $b\le e$ is tight when $m_i=1$.
\end{theorem}

Proof: We first start by looking at \saqpplusfunc, when answering an \avgfunc query.
From the results in the previous section, we know that:
\[ f(P_{\clean}) = d \cdot mean(\saqpplusfunc(P)) \]
\[ f(P_{\clean}) = d \cdot \frac{1}{|P|} \sum_p \frac{p_i}{m_i} \]
Similarly,
\[ mean(\saqpplusfunc^{1}(p)) = \frac{1}{|P|} \sum_p p_i \]
\[ d_2 \cdot mean(\saqpplusfunc^{2}(p)) = d_2 \cdot \frac{1}{|P|} \sum_p \frac{p_i}{m_i'} \]
Therefore,
\[ e = \frac{1}{|P|} |\frac{dp_i}{m_i}-{p_i}| \]
\[ b = \frac{1}{|P|} |\frac{dp_i}{m_i}-\frac{d_2p_i}{m_i'}| \]
Collecting terms:
\[ e = \frac{1}{|P|} |(\frac{d}{m_i}-1)p_i| \]
\[ b = \frac{1}{|P|} |(\frac{d}{m_i}-\frac{d_2}{m_i'})p_i| \]
since $1 \le m_i'$ and $d \ge d_2 \ge 1$, it follows that $b \le e$.
As $\frac{d}{m_i}-\frac{d_2}{m_i'} \rightarrow 0$, then we see that the
error is lower bounded by 0.
Removing the proportionality constants, this proof can be trivially extened to \sumfunc and \countfunc $\blacksquare$.



